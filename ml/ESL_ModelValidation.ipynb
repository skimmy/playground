{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation (ESL)\n",
    "This notebook expands on the concepts presented in chapeter 7 *Model Assessment and Selection* of the *Elements of Statistical Learning* book. The topics are not necessarily examined in order of appearence on the chapter, rather following a convenient thread of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-sample and In-sample errors\n",
    "It is interesting to elaborate the concept of *Extra-sample* and *In-sample* error covered in sections 7.4 and followinng of the chapter. Using the same notation of the books, let\n",
    "$$ \\mathcal{T} = \\{(x_1, y_1, \\ldots, x_N, y_N)\\} $$\n",
    "be the training set\n",
    "and considere a re-sampling of the points (here a make a little change in the notation my $y_i'$ corresponds to the $Y^0_i$ of the book).\n",
    "$$ \\mathcal{T}' = \\{ (x_1, y_i'), \\ldots, (x_N, y_N') \\} $$\n",
    "For a given error function $L(y_i, \\hat{f})$ we can calculate the *training error*\n",
    "$$ \\bar{err} = \\sum_{i=1}^{N}{L(y_i, \\hat{f}(x_i))} $$\n",
    "the *in-sample error*\n",
    "$$ Err_{in} = \\sum_{i=1}^{N}{L(y_i', \\hat{f}(x_i))} $$\n",
    "and the *optimism*\n",
    "$$ Err_{in} - \\bar{err} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error 663.2259363096656\n",
      "In sample mean error 899.9026352085289\n",
      "Average optimism 236.67669889886326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def linear_model(beta, X, noise=0):\n",
    "    '''Computes and returns beta.T*X + noise'''\n",
    "    return (np.matmul(X,beta) + noise*np.random.randn(np.shape(X)[0])).reshape(-1,1)\n",
    "\n",
    "n_samples = 50\n",
    "n_features = 1\n",
    "noise = 30\n",
    "X = np.random.randn(n_samples, n_features)*5\n",
    "beta  = np.random.randn(n_features)*40\n",
    "y = linear_model(beta,X,noise)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "tilde_y = model.predict(X)\n",
    "\n",
    "n_resamples = 30\n",
    "Y_res = np.zeros([n_samples, n_resamples])\n",
    "err_in_sample = np.zeros([n_resamples,1])\n",
    "# resample\n",
    "for i in range(n_resamples):\n",
    "    ys = linear_model(beta,X,noise)\n",
    "    Y_res[:,i] = ys.reshape(n_samples)\n",
    "    err_in_sample[i] = mean_squared_error(ys, model.predict(X))\n",
    "\n",
    "exp_in_sample = np.mean(err_in_sample)\n",
    "train_err = mean_squared_error(y, model.predict(X))\n",
    "print(\"Training error {0}\".format(train_err))\n",
    "print(\"In sample mean error {0}\".format(exp_in_sample))\n",
    "print(\"Average optimism {0}\".format(exp_in_sample-train_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have from the above analysis a partial confirmation that the optimism is, on average, positive and thus that the training error is somehow too good (*i.e.*, too optimistic) as predictor of the actual error. This is quite intuitive, but I don't see as obvious the analytical argument for it.\n",
    "\n",
    "It seems to me that the same phenomenon that makes training error biased downward, should be in place when we use training error as predictor of the test error. In this case the phenomenon should be even higher because we are not trying to estimate the same $x_i$ used for constructing the prediction, but we are trying to predict the unknown function in points $x'$ that (presumibly) we have not yet seen.\n",
    "\n",
    "The claim made in the text is that\n",
    "$$ E[Err_{in}-\\bar{err}] = \\frac{2}{N}\\sum_{i=1}^{N}{\\hat{y}_i, y_i)}$$\n",
    "The proof of this results (outlined in Exercise 7.4) could shed some lights on the nature of this situation and it is therefore useful to pursue to better understand the whole phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
