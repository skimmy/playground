{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-armed bandit\n",
    "In this notebook we familiarize with the problem known as $k$-armed bandit proble. The problem is the following, let $\\mathbf{B}_1,\\ldots,\\mathbf{B}_k$ be $k$ random process that can be *run* at any given time $t\\geq 0$. Let $R_i$ be the unknown *reward* of running process $i$. How can we maximize our total reward\n",
    "\\begin{align*}\n",
    "R = \\sum_{t=0}^{T}{R_{a(t)}}\n",
    "\\end{align*}\n",
    "when $T$ total run of any of the bandit are done? Notice that we initially do not know any of the returns, as we start pulling the arms we start to *learn* the rewards however we have to choose\n",
    "1. whether stick with the *best to time* arm or\n",
    "2. try a arm that we have not yet tried and see if we got higher rewards.\n",
    "The probal is more interesting when $T$ is of the same order of (or it is much less). In fact if $T \\gg k$ then a not-so-bad (although perhpas not optimal) strategy is to try all the arms in the first $k$ pulls, and then always pull the one that has the higher reward for the remaining $T-k$ pulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some experimentation\n",
    "We start with standard imports and define a function that generates the simplest bandits in forms of a $k$ vector of returns $\\mathbf{R} \\in \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_bandits(k, min_R=0, max_R=1, random_state=None):\n",
    "    if (random_state):\n",
    "        return np.random.RandomState(random_state).uniform(min_R, max_R, k)\n",
    "    return np.random.uniform(min_R, max_R, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit/Explore alternating strategy\n",
    "The first strategy we analyze to implement a solution strategy for the problem makes continuous alternation between *exploration* (try new arms) and *exploitation*. More specifically the algorithm works as follow\n",
    "1. Start with a random arm and pull it\n",
    "2. Save the pulled arm as the ``best_so_far``\n",
    "3. As long as there are unseen arms, choose one at random and pull it, if it rewards is better then the ``best_so_far`` use it a the new ``best_so_far``\n",
    "4. Perform an *exploit* pull of best so fare and then go back to 3\n",
    "5. If all arms have been pulled and there are stil steps to perform always pull ``best_so_far```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_strategy(bandits, steps, explore_rate=0.5):\n",
    "    rewards = np.zeros(steps)\n",
    "    t = 1\n",
    "    k = len(bandits)\n",
    "    unseen_arms = np.arange(k)\n",
    "    best_so_far = np.random.choice(unseen_arms)\n",
    "    np.delete(unseen_arms, best_so_far)\n",
    "    rewards[0] = bandits[best_so_far]\n",
    "    total_explore_steps = int(steps*explore_rate)\n",
    "    explore_step = 0\n",
    "    while(t < steps):\n",
    "        # Explore\n",
    "        if (explore_step < total_explore_steps):\n",
    "            new_arm = np.random.choice(unseen_arms)\n",
    "            np.delete(unseen_arms, new_arm)\n",
    "            rewards[t] = bandits[new_arm]\n",
    "            if (bandits[best_so_far] < bandits[new_arm]):\n",
    "                best_so_far = new_arm\n",
    "            t += 1\n",
    "        # Exploit\n",
    "        if (t < steps):\n",
    "            rewards[t] = bandits[best_so_far]\n",
    "            t += 1\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got reward 19263.6137 and best possible was 25563.3789\n",
      "Efficiency of strategy 75%\n"
     ]
    }
   ],
   "source": [
    "k = 2**10\n",
    "steps = int(k/4)\n",
    "bandits = make_bandits(k,0,100)\n",
    "actual_rewards = alternate_strategy(bandits, steps)\n",
    "obtained_reward = np.sum(actual_rewards)\n",
    "max_reward = steps*np.max(bandits)\n",
    "print(\"We got reward {0:.4f} and best possible was {1:.4f}\".format(obtained_reward, max_reward))\n",
    "print(\"Efficiency of strategy {0:.0f}%\".format(100*obtained_reward/max_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some comments on randomness\n",
    "We thought that using alternate random choices was better than any other way of selecting the next arm to explore, in fact this would be true if the arms were ordered in some specific way. In our setup, and in general, the arms have not any specific order, that is, they are already randomly shuffle, any other randomness is in fact useless. This means that the choices done in the explore stages of the above algorithm chould have been done linearly through the arms and the average return would not be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this strategy is not optimal, in fact this is due to the fact that no matter what, when ($T<k/2$) we always perform a total of $T/2$ exploration moves and save the ``best_so_far`` arm, however if we do all such moves at the begin of the run, we then are left with $T/2$ all on the ``best_of_all`` pulled arms. In other words the absolute best is not worse than any of the partial best.\n",
    "\n",
    "This implies that the strategy can be improved by making all the exploration pulls at the begin and then exploit the best arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_first_steategy(bandits, steps, explore_fraction=0.25):\n",
    "    rewards = np.zeros(steps)\n",
    "    explore_rounds = int(steps*explore_fraction)\n",
    "    exploit_rounds = steps - explore_rounds\n",
    "    # use a 'linear explore' strategy which works if 'bandits' is shuffled\n",
    "    best = 0\n",
    "    # first pull on the first arm\n",
    "    rewards[0] = bandits[0]\n",
    "    i = 1\n",
    "    best = 0\n",
    "    while i < explore_rounds:\n",
    "        rewards[i] = bandits[i]\n",
    "        if (bandits[i] > bandits[best]):\n",
    "            best = i\n",
    "        i += 1\n",
    "    # now exploit the best\n",
    "    while i < steps:\n",
    "        rewards[i] = bandits[best]\n",
    "        i += 1\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore first total reward 22157.13794\n",
      "Explore first efficiency 87%\n"
     ]
    }
   ],
   "source": [
    "explore_first_return = explore_first_steategy(bandits, steps)\n",
    "print(\"Explore first total reward {0:.5f}\".format(np.sum(explore_first_return)))\n",
    "print(\"Explore first efficiency {0:.0f}%\".format(np.sum(explore_first_return)*100/max_reward ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real testing\n",
    "We so far seen one single instance of the various model, in fact we always used the same ``bandits`` rewards to test the returns of the different models. This is of course to much dependent from the specific vector of rewards and therefore it is necessary to make some more robust testing performing several random experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1024 is out of bounds for axis 0 with size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3346a95f0c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# for alternate strategy it does not make sense to go beyond 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0malt_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternate_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mef_rewardws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplore_first_steategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mE_alt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mE_ef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mef_rewardws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-58886f3b9287>\u001b[0m in \u001b[0;36mexplore_first_steategy\u001b[0;34m(bandits, steps, explore_fraction)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mexplore_rounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbandits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbandits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1024 is out of bounds for axis 0 with size 1024"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "k = 100\n",
    "n_steps = 100*k    \n",
    "ratio = 0.5\n",
    "E_alt = np.array(n_samples)\n",
    "E_ef = np.array(n_samples)\n",
    "for i in range(n_samples):\n",
    "    bandit = make_bandits(k)\n",
    "    max_return = np.max(bandit)\n",
    "    ave_return = np.mean(bandit)\n",
    "    # for alternate strategy it does not make sense to go beyond 0.5\n",
    "    alt_rewards = alternate_strategy(bandits, n_steps, ratio)\n",
    "    ef_rewardws = explore_first_steategy(bandits, n_steps, ratio)\n",
    "    E_alt[i] = np.sum(alt_rewards)\n",
    "    E_ef[i] = np.sum(ef_rewardws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
